{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import requests as r\n",
    "import math\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import requests\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csvs_from_xls(filename):\n",
    "    wb = load_workbook(filename=filename)\n",
    "    problematic_sheets = {'policies'} # FIXME: remove after they are fixed and move to required_sheets\n",
    "    required_sheets = {'resources', 'technologies', 'tags'} \n",
    "    for idx, sheet_name in enumerate(wb.sheetnames, start=1):\n",
    "        sname = sheet_name.lower()\n",
    "        if sname in problematic_sheets:\n",
    "            print(f'NOTE: Data in {sheet_name} must be downloaded manually!!!!')\n",
    "            print('^' * 50)\n",
    "        if sname not in required_sheets:\n",
    "            continue\n",
    "        csv = f'{idx:-02}_{sheet_name.lower()}.csv'\n",
    "        df = pd.read_excel(filename, sheet_name=sheet_name)\n",
    "        df.columns = [c.strip().lower() for c in df.columns]\n",
    "        df.to_csv(csv, index=False)\n",
    "    return wb.sheetnames        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET ALL ORGANISTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_organisations():\n",
    "    orglist = []\n",
    "    for file in files:\n",
    "        data = pd.read_csv(file)\n",
    "        try:\n",
    "            orgs = list(data[data['organisation'] == data['organisation']]['organisation'])\n",
    "            for org in orgs:\n",
    "                org = org.replace('\"','').replace(',',';').split(';')\n",
    "                for og in org:\n",
    "                    og = og.strip()\n",
    "                    if og != '':\n",
    "                        orglist.append(og.strip())\n",
    "        except:\n",
    "            pass\n",
    "    orglist = list(np.unique(orglist))\n",
    "    orglist = [{\"name\":org} for org in orglist]\n",
    "    with open(output_folder + 'organisations.json', 'w') as f:\n",
    "        json.dump(orglist, f, indent=1, ensure_ascii=False)\n",
    "        \n",
    "    print(f'Updated {f.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory: LIST ALL MEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_regional_coverage():\n",
    "    #get unique reqgional coverage\n",
    "    mea = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        try:\n",
    "            df = df[df['geo_coverage'] == df['geo_coverage']]\n",
    "            df['geo_coverage_type'] = df['geo_coverage'].apply(lambda x: x.split(':')[0].lower() if ':' in x else np.nan)\n",
    "            df['geo_coverage'] = df['geo_coverage'].apply(lambda x: x.split(':')[1] if ':' in x else x)\n",
    "            df = df[df['geo_coverage_type'] == df['geo_coverage_type']]\n",
    "            df = df[['geo_coverage_type','geo_coverage']].to_dict('records')\n",
    "            for d in df:\n",
    "                if 'specific areas' in d['geo_coverage_type'] or d['geo_coverage_type'] == 'regional':\n",
    "                    if ';' in d['geo_coverage']:\n",
    "                        dd = d['geo_coverage'].split(';')\n",
    "                        for gc in dd:\n",
    "                            mea.append({'coverage':gc.strip(),'type': d['geo_coverage_type']})\n",
    "                    else:\n",
    "                        mea.append(mea.append({'coverage':d['geo_coverage'].strip(),'type':d['geo_coverage_type']}))\n",
    "        except:\n",
    "            print(file)\n",
    "    mea = [i for i in mea if i]\n",
    "    pd.DataFrame(mea).drop_duplicates(subset=['coverage'])\n",
    "    new = pd.read_csv('./country_group.csv')\n",
    "    new['source'] = 'new'\n",
    "    old = pd.DataFrame(r.get(\"http://unep.localhost/api/public/groups\").json())\n",
    "    old['source_api'] = 'unep.tc'\n",
    "    old = old[['name','source_api']]\n",
    "    new['duplicates'] = new['name'].apply(lambda x: old.loc[old['name'].str.contains(x)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET ALL TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tags():\n",
    "    tags = pd.read_csv('./06_tags.csv')\n",
    "    tags['category'] = tags['category'].apply(lambda x: x.split('_')[1])\n",
    "    tags = tags[['category','tag']]\n",
    "    tags = tags.groupby('category')['tag'].apply(lambda g: g.values.tolist()).to_dict()\n",
    "    with open(output_folder + 'tags.json', 'w') as f:\n",
    "        json.dump(tags, f, indent=1)\n",
    "    print(f'Updated {f.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET COUNTRY GROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_country_groups(data_file, country_group_names):\n",
    "    # FIXME: Not sure how to generate the country_group.json\n",
    "    # This code seems outdated, and we instead need country_group_countries\n",
    "    # groups = pd.read_csv('./09.country_groups.csv')\n",
    "    # groups['country'] = groups['country'].apply(lambda x: x.strip())\n",
    "    # countryGroup = groups.groupby('group')['country'].apply(lambda g: g.values.tolist()).to_dict()\n",
    "    # with open(output_folder + '/country_group.json', 'w') as file:\n",
    "    #    file.write(json.dumps(countryGroup, indent=1))\n",
    "    \n",
    "    groups = {}\n",
    "    for group_name in country_group_names:\n",
    "        df = pd.read_excel(data_file, sheet_name=group_name, squeeze=True, header=None)\n",
    "        groups[group_name] = df.apply(lambda x: x.strip()).to_list()\n",
    "    \n",
    "    with open(output_folder + 'country_group_countries.json', 'w') as f:\n",
    "        json.dump(groups, f, indent=1, ensure_ascii=False)\n",
    "    \n",
    "    print(f'Updated {f.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET ALL RESOURCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_currency(x):\n",
    "    if \"€\" in x or \"EUR\" in x:\n",
    "        return \"EUR\"\n",
    "    if \"$\" in x or \"USD\" in x:\n",
    "        return \"USD\"\n",
    "    if \"NOK\" in x:\n",
    "        return \"NOK\"\n",
    "    if \"CAD\" in x:\n",
    "        return \"CAD\"\n",
    "    if \"GBP\" in x or \"£\":\n",
    "        return \"GBP\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_currency(x):\n",
    "    cur = [int(s) for s in x.split() if s.isdigit()]\n",
    "    if len(cur) == 1:\n",
    "        if \"milli\" in x:\n",
    "            return cur[0] * 1000000\n",
    "        if \"billi\" in x:\n",
    "            return cur[0] * 1000000000\n",
    "        return cur[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_date_transformer(date):\n",
    "    if not date:\n",
    "        return None\n",
    "    if date == '18/18/2018':\n",
    "        return '2018-01-01'    \n",
    "    if '/' in date:\n",
    "        day, month, year = date.split('/')\n",
    "        return f'{year}-{month.zfill(2)}-{day.zfill(2)}'  \n",
    "    if len(date) == 4:\n",
    "        year = date\n",
    "        return f'{year}-01-01'\n",
    "    return date\n",
    "\n",
    "def generate_theme(theme):\n",
    "    converters = {\n",
    "        'first_publication_date': policy_date_transformer, \n",
    "        'latest_amendment_date': policy_date_transformer\n",
    "    }\n",
    "    srcs = pd.read_csv(theme, converters=converters)\n",
    "    taglist = pd.read_csv('./06_tags.csv')\n",
    "    taglist = [t.strip() for t in list(taglist['tag'])]\n",
    "    coverage_type = ['global',\n",
    "                     'regional',\n",
    "                     'national',\n",
    "                     'transnational',\n",
    "                     'sub-national',\n",
    "                     'global with elements in specific areas']\n",
    "    srcs = srcs.to_dict('records')\n",
    "    resources = []\n",
    "    false_tags = []\n",
    "    for src in srcs:\n",
    "        res = {}\n",
    "        for s in src:\n",
    "            res.update({s:src[s]})\n",
    "            if type(src[s]) == float:\n",
    "                if math.isnan(src[s]):\n",
    "                    res.update({s: None})\n",
    "            if s in [\"publish_year\",\"valid_from\",\"valid_to\"]:\n",
    "                if res[s] is not None:\n",
    "                    res.update({s: int(res[s])})\n",
    "            if s == \"url\":\n",
    "                if res[s] is not None:\n",
    "                    res.update({\"url\": [v.strip() for v in res[s].splitlines(True)]})\n",
    "            if s == \"languages\":\n",
    "                languages = []\n",
    "                if res[s] is not None:\n",
    "                    lang = []\n",
    "                    if \":\" in res[s]:\n",
    "                        lang = res[s].replace(\"http://\",\"\").replace(\"https://\",\"\").split(';')\n",
    "                    if len(lang) > 0:\n",
    "                        for ln in lang:\n",
    "                            ln = ln.split(\":\")\n",
    "                            if len(ln) > 1:\n",
    "                                languages.append({\"language\":ln[0].strip(),\"url\":\"https://{}\".format(ln[1].strip())})\n",
    "                if len(languages) > 0:\n",
    "                    res.update({\"resource_language_url\": languages})\n",
    "                else:\n",
    "                    res.update({\"resource_language_url\": None})\n",
    "                #del res[s]\n",
    "            if type(res[s]) == str:\n",
    "                v = res[s].replace('\\n','').replace('\"','').replace('‘','').replace('’','').replace('\\xa0',' ')\n",
    "                v = v.strip()\n",
    "                res.update({s: v})\n",
    "            ## Should we do data cleaning for value?\n",
    "            if s == \"value\":\n",
    "                if res[s] is not None:\n",
    "                    res.update({\"value_currency\": get_currency(res[s])})\n",
    "                    res.update({s: get_value_currency(res[s])})\n",
    "                else:\n",
    "                    res.update({\"value_currency\": None})\n",
    "            if s in [\"tags\",\"organisation\"]:\n",
    "                if type(res[s]) == str:\n",
    "                    vl = []\n",
    "                    if res[s] is not None:\n",
    "                        sep = [';' if ';' in src[s] else ':']\n",
    "                        vl = res[s].split(sep[0])\n",
    "                        vl = [k.replace('\"','').strip() for k in vl]\n",
    "                        if s == \"tags\":\n",
    "                            nv = []\n",
    "                            for tg in taglist:\n",
    "                                for v in vl:\n",
    "                                    if v == tg:\n",
    "                                        nv.append(v)\n",
    "                                    if v not in taglist:\n",
    "                                        false_tags.append(v)\n",
    "                            #if len(vl) != len(nv):\n",
    "                            #    res.update({\"error_tags\":True})\n",
    "                            #else:\n",
    "                            #    res.update({\"error_tags\":False})\n",
    "                            vl = nv\n",
    "                    if len(vl) == 0:\n",
    "                        vl = None\n",
    "                    res.update({s: vl})\n",
    "            if s == \"geo_coverage\":\n",
    "                if res[s] is not None:\n",
    "                    gt = res[s].split(\":\")\n",
    "                    ct = gt[0].lower().strip()\n",
    "                    if ct in coverage_type:\n",
    "                        res.update({'geo_coverage_type': ct})\n",
    "                    else:\n",
    "                        res.update({'geo_coverage_type': None})\n",
    "                    if len(gt) > 1:\n",
    "                        gc = gt[1].split(';')\n",
    "                        gc = [g.replace('.','').strip() for g in gc]\n",
    "                        res.update({s: gc})\n",
    "                    else:\n",
    "                        res.update({s: None})\n",
    "            if s == \"attachments\":\n",
    "                if res[s] is not None:\n",
    "                    res.update({s: res[s].split(' ')})\n",
    "                else:\n",
    "                    res.update({s: []})\n",
    "            if s == \"country\":\n",
    "                if res[s] is not None:\n",
    "                    if \",\" in res[s]:\n",
    "                        country = res[s].split(',')\n",
    "                    elif \";\" in res[s]:\n",
    "                        country = res[s].split(';')\n",
    "                    else:\n",
    "                        country = [res[s]]\n",
    "                    res.update({\"country\": country[0]})\n",
    "                else:\n",
    "                    res.update({\"country\": None})\n",
    "        resources.append(res)\n",
    "    results = pd.DataFrame(resources).to_dict('records')\n",
    "    #df = df.fillna(dict(publish_year=999)).replace(dict(publish_year={999: None}))\n",
    "    for res in results:\n",
    "        for s in res:\n",
    "            if type(res[s]) == float:\n",
    "                if math.isnan(res[s]):\n",
    "                    res.update({s: None})\n",
    "                else:\n",
    "                    res.update({s: int(res[s])})\n",
    "    resources = pd.DataFrame(results)\n",
    "    output_file = theme.split('_', 1)[1].replace('.csv','.json')\n",
    "    with open(output_folder + output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4 if 'resources' in theme else 1)\n",
    "    print(f'Updated {f.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_countries(): \n",
    "    with open('countries.geojson', 'r') as f:\n",
    "        country_data = json.load(f)\n",
    "        \n",
    "    countries = {}\n",
    "    for feature in country_data['features']:\n",
    "        props = feature['properties']\n",
    "        name = props['name']\n",
    "        if name.startswith('disputed'):\n",
    "            continue\n",
    "        if name not in countries:\n",
    "            countries[name] = props['cd']\n",
    "        elif 'Island' in countries[name] and 'Island' not in props['cd']:\n",
    "            countries[name] = props['cd']\n",
    "        else:\n",
    "            print(f'{name} already mapped to {countries[name]}. Not adding {props}')\n",
    "            \n",
    "    countries = [{\"name\": val, \"code\": key} for key, val in countries.items()]\n",
    "    countries = countries + [{'name': \"All\", \"code\": None}, {'name': \"Other\", \"code\": None}]\n",
    "        \n",
    "    with open(output_folder + 'countries.json', 'w') as f:\n",
    "        json.dump(countries, f, indent=4, ensure_ascii=False)\n",
    "    print(f'Updated {f.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import/Update Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = json.loads(requests.get('https://unep.tc.akvo.org/api/export/project-actions').content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    if action['parent_id'] == 116:\n",
    "        print(f\"{action['code']}: '{action['name'].lower().split('(')[0].strip()}',\")\n",
    "        \n",
    "geo_coverage_codes = {\n",
    "    105885227: 'global',\n",
    "    105885347: 'regional',\n",
    "    105885443: 'transnational',\n",
    "    105885568: 'national',\n",
    "    105885616: 'sub-national',\n",
    "    999999001: 'global with elements in specific areas',\n",
    "    105994502: 'other',\n",
    "}\n",
    "geo_coverage_names = {val: key for key, val in geo_coverage_codes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_project_geo_coverage(project):\n",
    "    countries = set(project['countries'])\n",
    "    project_action_codes = set(project['action_codes'])\n",
    "    \n",
    "    if 'All' in countries:\n",
    "        geo_coverage_type = 'global'\n",
    "        project['countries'] = []\n",
    "        \n",
    "    elif len(countries) > 1 or geo_coverage_names['transnational'] in project_action_codes:\n",
    "        geo_coverage_type = 'transnational'\n",
    "        \n",
    "    elif len(countries) == 1:\n",
    "        if {geo_coverage_names['national'], geo_coverage_names['regional'], geo_coverage_names['other']}.intersection(project_action_codes):\n",
    "            geo_coverage_type = 'national'\n",
    "        elif geo_coverage_names['sub-national'] in project_action_codes:\n",
    "            geo_coverage_type = 'sub-national'\n",
    "        elif \"Narrative Submission\" in project['title']:\n",
    "            geo_coverage_type = 'national'\n",
    "        elif 'Other' in project['countries'] and geo_coverage_names['global'] in project_action_codes:\n",
    "            geo_coverage_type = 'global'\n",
    "            project['countries'] = []\n",
    "        else:\n",
    "            print(\"1 country project!!!\")\n",
    "            pprint(project['title'])\n",
    "            pprint(project['countries'])\n",
    "    else:\n",
    "        geo_coverage_type = None\n",
    "        #print(\"No countries??\")\n",
    "        #pprint(project['title'])\n",
    "    project['geo_coverage_type'] = geo_coverage_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def update_projects():\n",
    "    project_data = json.loads(requests.get('https://unep.tc.akvo.org/api/export/projects').content)\n",
    "    country_data = json.loads(requests.get('https://unep.tc.akvo.org/public/api/countries').content)\n",
    "\n",
    "    TITLE_ACTION_CODE = 43374800\n",
    "    SUMMARY_ACTION_CODE = 43374829\n",
    "    \n",
    "    old_names = {c['name']: c['code'] for c in country_data}\n",
    "    with open(output_folder + 'countries.json', 'r') as f:\n",
    "        new_names = {c['code']: c['name'] for c in json.load(f)}\n",
    "\n",
    "    for project in project_data:\n",
    "        for action_detail in project['action_details']:\n",
    "            if action_detail['action_detail_code'] == TITLE_ACTION_CODE:\n",
    "                title = action_detail['value']\n",
    "                # Handle one ugly piece of data\n",
    "                if title.startswith('1.\\t'):\n",
    "                    title = title[3:]\n",
    "                project['title'] = title\n",
    "            elif action_detail['action_detail_code'] == SUMMARY_ACTION_CODE:\n",
    "                summary = action_detail['value']\n",
    "                project['summary'] = summary\n",
    "       \n",
    "        # Replace \"old\" country names with new country names\n",
    "        countries = []\n",
    "        for name in project['countries']:\n",
    "            if name not in old_names:\n",
    "                print(project)\n",
    "                print(f\"{name} missing in old country list!\")\n",
    "                continue\n",
    "            code = old_names[name]\n",
    "            if code is not None:\n",
    "                new_name = new_names.get(code)\n",
    "                if new_name is None:\n",
    "                    print(f\"{code} ({name}) not found in new country name list\")\n",
    "                    continue\n",
    "            else:\n",
    "                new_name = name\n",
    "            countries.append(new_name)\n",
    "        project['countries'] = countries\n",
    "\n",
    "        transform_project_geo_coverage(project)\n",
    "\n",
    "    with open('../backend/dev/resources/files/projects.json', 'w') as f:\n",
    "        json.dump(project_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f'Updated {f.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'flat-data-structure.xlsx'\n",
    "output_folder = \"../backend/dev/resources/files/\"\n",
    "sheetnames = create_csvs_from_xls(data_file)\n",
    "files = glob.glob(\"./0*.csv\")\n",
    "print(files)\n",
    "\n",
    "update_countries()  # NOTE: This is slightly slower, because we open the huge geojson\n",
    "update_organisations()\n",
    "update_tags()\n",
    "country_group_names = sheetnames[sheetnames.index('projects')+1:]\n",
    "update_country_groups(data_file, country_group_names)\n",
    "generate_theme(\"05_technologies.csv\")\n",
    "generate_theme(\"04_policies.csv\")\n",
    "generate_theme(\"02_resources.csv\")\n",
    "# NOTE: Always call update_projects after update_countries\n",
    "update_projects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTES\n",
    "- Do we need to analyze value and it's currency?\n",
    "- No Country Table\n",
    "- Country also has **global with elements in specific areas**\n",
    "- Some of the country has different separator\n",
    "- Some of the geo_coverage has different separator\n",
    "- Some of the tags has different separator\n",
    "- languages separator is using colon while the url is also using colon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
