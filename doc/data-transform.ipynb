{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import requests as r\n",
    "import math\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import requests\n",
    "from openpyxl import load_workbook\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketurl = \"https://storage.googleapis.com/akvo-unep-gpml\"\n",
    "#API_URL = \"https://unep.tc.akvo.org\"\n",
    "API_URL = \"http://unep.localhost\"\n",
    "output_folder = \"../backend/dev/resources/files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPDATE COUNTRY GROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_country_groups():\n",
    "    country_group_countries = open(output_folder + 'country_group_countries.json')\n",
    "    country_group_countries = json.load(country_group_countries)\n",
    "    country_group_countries_old = r.get(API_URL + '/api/export/groups').json()\n",
    "    country_groups = open(output_folder + 'country_group.json')\n",
    "    country_groups = json.load(country_groups)\n",
    "    country_groups = pd.DataFrame(country_groups)\n",
    "    last_cg_id = list(country_groups.tail(1)['id'])[0]\n",
    "    new_country_groups = []\n",
    "    for cgc in country_group_countries_old:\n",
    "        if cgc not in list(country_group_countries):\n",
    "            country_group_countries.update({cgc: country_group_countries_old[cgc]})\n",
    "        if cgc not in list(country_groups['name']):\n",
    "            last_cg_id = last_cg_id + 1;\n",
    "            regional = ['Black Sea','South Asia Seas','South East Pacific','Pacific','Artic','Antarctic','Baltic Sea','North East Pacific']\n",
    "            rtype = 'region'\n",
    "            if cgc not in regional:\n",
    "                rtype = 'mea'\n",
    "            new_country_groups.append({'id': last_cg_id,'name':cgc,'type':rtype})\n",
    "    if len(new_country_groups) > 0:\n",
    "        country_groups = country_groups.append(new_country_groups, ignore_index=True)\n",
    "        \n",
    "    for cs in list(country_group_countries):\n",
    "        if cs not in list(country_groups['name']):\n",
    "            print(cs)\n",
    "\n",
    "    with open(output_folder + 'country_group_countries.json', 'w') as f:\n",
    "        json.dump(country_group_countries, f, indent=2, ensure_ascii=False)\n",
    "        print(f'Updated {f.name}')\n",
    "\n",
    "    with open(output_folder + 'country_group.json', 'w') as f:\n",
    "        json.dump(country_groups.to_dict('records'), f, indent=2, ensure_ascii=False)\n",
    "        print(f'Updated {f.name}')\n",
    "        \n",
    "    return country_group_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectCountries(cgname):\n",
    "    if cgname in list(country_group_countries):\n",
    "        return country_group_countries[cgname]\n",
    "    return [cgname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csvs_from_xls(filename):\n",
    "    wb = load_workbook(filename=filename)\n",
    "    problematic_sheets = {'policies'} # FIXME: remove after they are fixed and move to required_sheets\n",
    "    required_sheets = {'events','resources', 'technologies', 'tags'} \n",
    "    for idx, sheet_name in enumerate(wb.sheetnames, start=1):\n",
    "        sname = sheet_name.strip().lower()\n",
    "        if sname in problematic_sheets:\n",
    "            print(f'NOTE: Data in {sheet_name} must be downloaded manually!!!!')\n",
    "            print('^' * 50)\n",
    "        if sname not in required_sheets:\n",
    "            continue\n",
    "        csv = f'{idx:-02}_{sname}.csv'\n",
    "        df = pd.read_excel(filename, sheet_name=sheet_name)\n",
    "        df.columns = [c.strip().lower() for c in df.columns]\n",
    "        df.to_csv(csv, index=False)\n",
    "    return wb.sheetnames        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET ALL ORGANISTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_organisations():\n",
    "    orglist = []\n",
    "    for file in files:\n",
    "        data = pd.read_csv(file)\n",
    "        try:\n",
    "            orgs = list(data[data['organisation'] == data['organisation']]['organisation'])\n",
    "            for org in orgs:\n",
    "                org = org.replace('\"','').replace(',',';').split(';')\n",
    "                for og in org:\n",
    "                    og = og.strip()\n",
    "                    if og != '':\n",
    "                        orglist.append(og.strip())\n",
    "        except:\n",
    "            pass\n",
    "    orglist = list(np.unique(orglist))\n",
    "    orglist = [{\"name\":org} for org in orglist]\n",
    "    with open(output_folder + 'organisations.json', 'w') as f:\n",
    "        json.dump(orglist, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    print(f'Updated {f.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET ALL TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tags():\n",
    "    tags = pd.read_csv('./06_tags.csv', skip_blank_lines=True)\n",
    "    tags = tags[~tags['category'].isna()]\n",
    "    tags['category'] = tags['category'].apply(lambda x: x.split('_')[1])\n",
    "    tags = tags[['category','tag']]\n",
    "    tags = tags.groupby('category')['tag'].apply(lambda g: g.values.tolist()).to_dict()\n",
    "    with open(output_folder + 'tags.json', 'w') as f:\n",
    "        json.dump(tags, f, indent=2, ensure_ascii=False)\n",
    "    print(f'Updated {f.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET ALL RESOURCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_currency(x):\n",
    "    if \"€\" in x or \"EUR\" in x:\n",
    "        return \"EUR\"\n",
    "    if \"$\" in x or \"USD\" in x:\n",
    "        return \"USD\"\n",
    "    if \"NOK\" in x:\n",
    "        return \"NOK\"\n",
    "    if \"CAD\" in x:\n",
    "        return \"CAD\"\n",
    "    if \"GBP\" in x or \"£\":\n",
    "        return \"GBP\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_currency(x):\n",
    "    cur = [int(s) for s in x.split() if s.isdigit()]\n",
    "    if len(cur) == 1:\n",
    "        if \"milli\" in x:\n",
    "            return cur[0] * 1000000\n",
    "        if \"billi\" in x:\n",
    "            return cur[0] * 1000000000\n",
    "        return cur[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images():\n",
    "    images = glob.glob(\"./images/**/*.*\")\n",
    "    images_map = {}\n",
    "    for image in images:\n",
    "        basename = os.path.basename(image)\n",
    "        name = os.path.splitext(basename)[0]\n",
    "        is_logo = name.endswith('logo')\n",
    "        topic, topic_id = name.split('_')[:2]\n",
    "        try:\n",
    "            topic_id = int(topic_id)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        topic_map = images_map.setdefault(topic, {})\n",
    "        topic_id_map = topic_map.setdefault(topic_id, {})\n",
    "        key = 'logo' if is_logo else 'image'\n",
    "        topic_id_map[key] = f\"{bucketurl}/images/{basename}\"\n",
    "        \n",
    "    if not images_map:\n",
    "        raise RuntimeError('No images found')\n",
    "\n",
    "    return images_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revertGeoToBeforeFormat(x):\n",
    "    # IMPORTANT NOTES, Because the format before in geo_coverage column is 'geo_coverage_type':'geo_coverage_values'\n",
    "    # Since 24th February, Unep manage to change the format to move geo_coverage_values to country\n",
    "    # so we revert to how it's before to prevent code changed\n",
    "    # If we are using old format as above, then we need to comment this function inside generate_theme\n",
    "    if x['country'] == x['country'] and x['geo_coverage'] == x['geo_coverage']:\n",
    "        return x['geo_coverage'] + ':' + x['country']\n",
    "    return x['geo_coverage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_date_transformer(date):\n",
    "    if not date:\n",
    "        return None\n",
    "    if date == '18/18/2018':\n",
    "        return '2018-01-01'    \n",
    "    if '/' in date:\n",
    "        day, month, year = date.split('/')\n",
    "        return f'{year}-{month.zfill(2)}-{day.zfill(2)}'  \n",
    "    if len(date) == 4:\n",
    "        year = date\n",
    "        return f'{year}-01-01'\n",
    "    return date\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_theme(theme):\n",
    "    converters = {\n",
    "        'first_publication_date': policy_date_transformer, \n",
    "        'latest_amendment_date': policy_date_transformer,\n",
    "        'start_date': policy_date_transformer,\n",
    "        'end_date': policy_date_transformer\n",
    "    }\n",
    "    srcs = pd.read_csv(theme, converters=converters)\n",
    "    if 'country' not in list(srcs):\n",
    "        srcs['country'] = ''\n",
    "    srcs['geo_coverage'] = srcs.apply(lambda x: revertGeoToBeforeFormat(x),axis=1)\n",
    "    # Drop unnamed columns\n",
    "    columns = [col for col in srcs.columns if not col.startswith('unnamed')]\n",
    "    srcs = srcs[columns]\n",
    "    # Drop rows with no data\n",
    "    srcs = srcs[~srcs['id'].isna()]\n",
    "    taglist = pd.read_csv('./06_tags.csv')\n",
    "    taglist = taglist[~taglist['tag'].isna()]\n",
    "    taglist = [t.strip() for t in list(taglist['tag'])]\n",
    "    coverage_type = ['global',\n",
    "                     'regional',\n",
    "                     'national',\n",
    "                     'transnational',\n",
    "                     'sub-national',\n",
    "                     'global with elements in specific areas']\n",
    "    srcs = srcs.to_dict('records')\n",
    "    resources = []\n",
    "    false_tags = []\n",
    "    for src in srcs:\n",
    "        res = {}\n",
    "        for s in src:\n",
    "            res.update({s:src[s]})\n",
    "            if type(src[s]) == float:\n",
    "                if math.isnan(src[s]):\n",
    "                    res.update({s: None})\n",
    "            if s in [\"publish_year\",\"valid_from\",\"valid_to\"]:\n",
    "                if res[s] is not None:\n",
    "                    res.update({s: res[s]})\n",
    "            if s == \"url\":\n",
    "                if res[s] is not None:\n",
    "                    res.update({\"url\": [v.strip() for v in res[s].split(\";\")]})\n",
    "            if s == \"languages\":\n",
    "                languages = []\n",
    "                res.update({s:src[s]})\n",
    "                urls = []\n",
    "                lang = ['English']\n",
    "                if src['url'] == src['url']:\n",
    "                    urls = src['url'].replace(\"http://\",\"\").replace(\"https://\",\"\").split(';')\n",
    "                    if src['languages'] == src['languages']:\n",
    "                        if \";\" in src['languages']:\n",
    "                            lang = src['languages'].split(';')\n",
    "                        else:\n",
    "                            lang = [src['languages'].split(':')[0]]\n",
    "                    if len(urls) > len(lang):\n",
    "                        for i, u in enumerate(urls):\n",
    "                            if len(lang) > 0:\n",
    "                                lang.append(lang[0])\n",
    "                            else:\n",
    "                                lang.append(\"English\")\n",
    "                    if len(lang) > 0:\n",
    "                        for i, lan in enumerate(lang):\n",
    "                            try:\n",
    "                                languages.append({\"language\":lan.strip(),\"url\":\"https://{}\".format(urls[i].strip())})\n",
    "                            except:\n",
    "                                pass\n",
    "                if len(languages) > 0:\n",
    "                    res.update({\"resource_language_url\": languages})\n",
    "                else:\n",
    "                    res.update({\"resource_language_url\": None})\n",
    "                #del res[s]\n",
    "            if type(res[s]) == str:\n",
    "                v = res[s].replace('\\n','').replace('\"','').replace('‘','').replace('’','').replace('\\xa0',' ')\n",
    "                v = v.strip()\n",
    "                res.update({s: v})\n",
    "            ## Should we do data cleaning for value?\n",
    "            if s == \"value\":\n",
    "                if res[s] is not None:\n",
    "                    res.update({\"value_currency\": get_currency(res[s])})\n",
    "                    res.update({s: get_value_currency(res[s])})\n",
    "                else:\n",
    "                    res.update({\"value_currency\": None})\n",
    "            if s == \"value_amount\":\n",
    "                res.update({\"value_amount\": src[s]})\n",
    "            if s in [\"tags\",\"organisation\"]:\n",
    "                if type(res[s]) == str:\n",
    "                    vl = []\n",
    "                    if res[s] is not None:\n",
    "                        sep = [';' if ';' in src[s] else ':']\n",
    "                        vl = res[s].split(sep[0])\n",
    "                        vl = [k.replace('\"','').strip() for k in vl]\n",
    "                        if s == \"tags\":\n",
    "                            nv = []\n",
    "                            for tg in taglist:\n",
    "                                for v in vl:\n",
    "                                    if v == tg:\n",
    "                                        nv.append(v)\n",
    "                                    if v not in taglist:\n",
    "                                        false_tags.append(v)\n",
    "                            vl = nv\n",
    "                    if len(vl) == 0:\n",
    "                        vl = None\n",
    "                    res.update({s: vl})\n",
    "            if s == \"geo_coverage\":\n",
    "                if res[s] is not None:\n",
    "                    gt = res[s].split(\":\")\n",
    "                    ct = gt[0].lower().strip()\n",
    "                    if ct in coverage_type:\n",
    "                        res.update({'geo_coverage_type': ct})\n",
    "                    else:\n",
    "                        res.update({'geo_coverage_type': None})\n",
    "                    if len(gt) > 1:\n",
    "                        gc = gt[1].split(';')\n",
    "                        gc = [g.replace('.','').strip() for g in gc]\n",
    "                        if ct == 'transnational':\n",
    "                            for g in gc:\n",
    "                                cc = collectCountries(g)\n",
    "                                for c in cc:\n",
    "                                    if c.strip() not in gc:\n",
    "                                        gc.append(c)\n",
    "                        res.update({s: gc})\n",
    "                    else:\n",
    "                        res.update({s: None})\n",
    "            if s == \"attachments\":\n",
    "                if res[s] is not None:\n",
    "                    res.update({s: res[s].split(' ')})\n",
    "                else:\n",
    "                    res.update({s: []})\n",
    "            if s == \"country\":\n",
    "                if res[s] is not None:\n",
    "                    if \";\" in res[s]:\n",
    "                        country = res[s].split(';')\n",
    "                    else:\n",
    "                        country = [res[s]]\n",
    "                    res.update({\"country\": country[0]})\n",
    "                else:\n",
    "                    res.update({\"country\": None})\n",
    "        resources.append(res)\n",
    "    results = pd.DataFrame(resources)\n",
    "    images = get_images()\n",
    "    topic_name = theme.replace('ies.csv','y').replace('s.csv','').split('_')[1]\n",
    "    if topic_name == 'event':\n",
    "        topic_name = 'events'\n",
    "    results['image'] = results['id'].apply(lambda x: images.get(topic_name, {}).get(int(x), {}).get('image'))\n",
    "    if topic_name == 'technology':\n",
    "        results['logo'] = results['id'].apply(lambda x: images.get(topic_name, {}).get(int(x), {}).get('logo'))\n",
    "    results = results.to_dict('records')\n",
    "    for res in results:\n",
    "        for s in res:\n",
    "            if type(res[s]) == float:\n",
    "                if math.isnan(res[s]):\n",
    "                    res.update({s: None})\n",
    "                else:\n",
    "                    res.update({s: int(res[s])})\n",
    "    resources = pd.DataFrame(results)\n",
    "    output_file = theme.split('_', 1)[1].replace('.csv','.json')\n",
    "    with open(output_folder + output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f'Updated {f.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_countries(): \n",
    "    with open('countries.geojson', 'r') as f:\n",
    "        country_data = json.load(f)\n",
    "        \n",
    "    countries = {}\n",
    "    for feature in country_data['features']:\n",
    "        props = feature['properties']\n",
    "        name = props['name']\n",
    "        if name.startswith('disputed'):\n",
    "            continue\n",
    "        if name not in countries:\n",
    "            countries[name] = props['cd']\n",
    "        elif 'Island' in countries[name] and 'Island' not in props['cd']:\n",
    "            countries[name] = props['cd']\n",
    "        else:\n",
    "            print(f'{name} already mapped to {countries[name]}. Not adding {props}')\n",
    "            \n",
    "    countries = [{\"name\": val, \"code\": key} for key, val in countries.items()]\n",
    "    countries = countries + [{'name': \"All\", \"code\": None}, {'name': \"Other\", \"code\": None}]\n",
    "        \n",
    "    with open(output_folder + 'countries.json', 'w') as f:\n",
    "        json.dump(countries, f, indent=2, ensure_ascii=False)\n",
    "    print(f'Updated {f.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import/Update Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = json.loads(requests.get(API_URL + '/api/export/project-actions').content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    if action['parent_id'] == 116:\n",
    "        print(f\"{action['code']}: '{action['name'].lower().split('(')[0].strip()}',\")\n",
    "        \n",
    "geo_coverage_codes = {\n",
    "    105885227: 'global',\n",
    "    105885347: 'regional',\n",
    "    105885443: 'transnational',\n",
    "    105885568: 'national',\n",
    "    105885616: 'sub-national',\n",
    "    999999001: 'global with elements in specific areas',\n",
    "    105994502: 'other',\n",
    "}\n",
    "geo_coverage_names = {val: key for key, val in geo_coverage_codes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_project_geo_coverage(project):\n",
    "    countries = set(project['countries'])\n",
    "    project_action_codes = set(project['action_codes'])\n",
    "    geo_coverage_type = None\n",
    "    if 'All' in countries:\n",
    "        geo_coverage_type = 'global'\n",
    "        project['countries'] = []\n",
    "        \n",
    "    elif len(countries) > 1 or geo_coverage_names['transnational'] in project_action_codes:\n",
    "        geo_coverage_type = 'transnational'\n",
    "        \n",
    "    elif len(countries) == 1:\n",
    "        if {geo_coverage_names['national'], geo_coverage_names['regional'], geo_coverage_names['other']}.intersection(project_action_codes):\n",
    "            geo_coverage_type = 'national'\n",
    "        elif geo_coverage_names['sub-national'] in project_action_codes:\n",
    "            geo_coverage_type = 'sub-national'\n",
    "        elif \"Narrative Submission\" in project['title']:\n",
    "            geo_coverage_type = 'national'\n",
    "        elif 'Other' in project['countries'] and geo_coverage_names['global'] in project_action_codes:\n",
    "            geo_coverage_type = 'global'\n",
    "            project['countries'] = []\n",
    "        else:\n",
    "            #print(\"1 country project!!!\")\n",
    "            #pprint(project['title'])\n",
    "            #pprint(project['countries'])\n",
    "            pass\n",
    "    else:\n",
    "        geo_coverage_type = None\n",
    "        #print(\"No countries??\")\n",
    "        #pprint(project['title'])\n",
    "    project['geo_coverage_type'] = geo_coverage_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def update_projects():\n",
    "    project_data = json.loads(requests.get(API_URL + '/api/export/projects').content)\n",
    "    country_data = json.loads(requests.get(API_URL + '/api/countries').content)\n",
    "    \n",
    "    files = \"./images/activity/activity_\"\n",
    "    activity_images = glob.glob(\"{}*\".format(files))\n",
    "    available_images = []\n",
    "    for image in activity_images:\n",
    "        root, ext = os.path.splitext(image)\n",
    "        available_images.append({'uuid': root.replace(files,''),'ext': ext})\n",
    "    for p in project_data:\n",
    "        ext = filter(lambda i: p['uuid'] == i['uuid'] , available_images)\n",
    "        image = list(ext)\n",
    "        if len(image) > 0:\n",
    "            image = image[0]\n",
    "            image = \"{}/images/activity_{}{}\".format(bucketurl, image['uuid'], image['ext'])\n",
    "            p.update({'image':image})\n",
    "        else:\n",
    "            p.update({'image':None})\n",
    "\n",
    "    TITLE_ACTION_CODE = 43374800\n",
    "    SUMMARY_ACTION_CODE = 43374829\n",
    "    URL_ACTION_CODE = 43374839\n",
    "    \n",
    "    URL_REGEX = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    \n",
    "    old_names = {c['name']: c['code'] for c in country_data}\n",
    "    with open(output_folder + 'countries.json', 'r') as f:\n",
    "        new_names = {c['code']: c['name'] for c in json.load(f)}\n",
    "\n",
    "    for project in project_data:\n",
    "        project['url'] = None\n",
    "        for action_detail in project['action_details']:\n",
    "            if action_detail['action_detail_code'] == TITLE_ACTION_CODE:\n",
    "                title = action_detail['value']\n",
    "                # Handle one ugly piece of data\n",
    "                if title.startswith('1.\\t'):\n",
    "                    title = title[3:]\n",
    "                project['title'] = title\n",
    "            elif action_detail['action_detail_code'] == SUMMARY_ACTION_CODE:\n",
    "                summary = action_detail['value']\n",
    "                project['summary'] = summary\n",
    "            elif action_detail['action_detail_code'] == URL_ACTION_CODE:\n",
    "                url = action_detail['value']\n",
    "                url = re.findall(URL_REGEX, url)\n",
    "                if len(url) > 0:\n",
    "                    project['url'] = \"https://\" + url[0][0].replace(\"https://\",\"\").replace(\"http://\",\"\")\n",
    "       \n",
    "        # Replace \"old\" country names with new country names\n",
    "        countries = []\n",
    "        for name in project['countries']:\n",
    "            if name not in old_names:\n",
    "                print(project)\n",
    "                print(f\"{name} missing in old country list!\")\n",
    "                continue\n",
    "            code = old_names[name]\n",
    "            if code is not None:\n",
    "                new_name = new_names.get(code)\n",
    "                if new_name is None:\n",
    "                    print(f\"{code} ({name}) not found in new country name list\")\n",
    "                    continue\n",
    "            else:\n",
    "                new_name = name\n",
    "            countries.append(new_name)\n",
    "        project['countries'] = countries\n",
    "\n",
    "        transform_project_geo_coverage(project)\n",
    "\n",
    "    with open('../backend/dev/resources/files/projects.json', 'w') as f:\n",
    "        json.dump(project_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f'Updated {f.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_file = 'flat data-structure_2021-04-13.xlsx'\n",
    "sheetnames = create_csvs_from_xls(data_file)\n",
    "files = glob.glob(\"./0*.csv\")\n",
    "print(files)\n",
    "\n",
    "# update_countries()  # NOTE: This is slightly slower, because we open the huge geojson\n",
    "# update_organisations()\n",
    "# update_tags()\n",
    "country_group_countries = update_country_groups()\n",
    "generate_theme('01_resources.csv')\n",
    "generate_theme('03_events.csv')\n",
    "generate_theme('04_policies.csv')\n",
    "generate_theme('05_technologies.csv')\n",
    "# NOTE: Always call update_projects after update_countries\n",
    "# update_projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datetime.strptime('04/03/2021', '%d/%m/%Y').strftime(\"%m/%d/%Y, %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTES\n",
    "- Do we need to analyze value and it's currency?\n",
    "- No Country Table\n",
    "- Country also has **global with elements in specific areas**\n",
    "- Some of the country has different separator\n",
    "- Some of the geo_coverage has different separator\n",
    "- Some of the tags has different separator\n",
    "- languages separator is using colon while the url is also using colon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
